{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Saudi E-Commerce RAG Agent\n",
        "\n",
        "This notebook implements a Retrieval-Augmented Generation (RAG) agent specifically designed for Saudi e-commerce data. The goal is to enable intelligent question‚Äìanswering and insights generation using product descriptions, metadata, and domain knowledge.\n",
        "\n",
        "**Features:**\n",
        "- Embedding-based document indexing  \n",
        "- Vector similarity search  \n",
        "- LLM-based question answering  \n",
        "- RAG pipeline for context-grounded responses  \n",
        "\n",
        "**Technologies used:**\n",
        "- Python\n",
        "- FAISS / Chroma / (your vector store)\n",
        "- OpenAI / Llama / other model\n",
        "- LangChain / LlamaIndex (if applicable)\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "wgkuLB8lMCdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install & Import Libraries"
      ],
      "metadata": {
        "id": "IKHNdmxkMOXC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJYlvvq0Sk3s"
      },
      "outputs": [],
      "source": [
        "!pip install langchain chromadb sentence-transformers transformers pypdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.listdir()\n"
      ],
      "metadata": {
        "id": "87vv9nJVTh8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community\n"
      ],
      "metadata": {
        "id": "afQzz2__TiCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n"
      ],
      "metadata": {
        "id": "JyaRPWNYUOIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    \"8340436664-publichedpaper.pdf\",\n",
        "    \"V5.0 Monsha'at SMEM Report - Q2-25.pdf\",\n",
        "    \"monsha‚Äôat_e_commerce_thematic_report_en.pdf\",\n",
        "    \"3-Presentation-by-Kingdom-of-Saudi-Arabia.pdf\"\n",
        "]\n",
        "\n",
        "documents = []\n",
        "\n",
        "for file in files:\n",
        "    loader = PyPDFLoader(file)\n",
        "    docs = loader.load()\n",
        "    documents.extend(docs)\n",
        "\n",
        "len(documents)\n"
      ],
      "metadata": {
        "id": "VHRC0on6UOMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "gQMpDpBjVDRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding Model and Vector Store Construction\n",
        "\n",
        "In this section, we initialize the embedding model and build the vector database that powers semantic search.\n",
        "\n",
        "### **HuggingFace Embeddings**\n",
        "We use the `all-MiniLM-L6-v2` model from SentenceTransformers, a lightweight and fast model suitable for semantic similarity tasks. This model converts each text chunk into a dense numerical vector (embedding).\n",
        "\n",
        "### **Chroma Vector Store**\n",
        "Chroma is used as the vector database to store embeddings and enable efficient similarity search.  \n",
        "Each product description chunk is embedded and stored inside Chroma for retrieval.\n",
        "\n",
        "### **Retriever**\n",
        "After building the vector store, we convert it into a retriever. The retriever will:\n",
        "- Accept a user query\n",
        "- Embed the query\n",
        "- Find the top similar chunks from the vectorstore\n",
        "- Return them to the RAG agent as context\n"
      ],
      "metadata": {
        "id": "-ZYI8Sdzh-h-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# Recreate embedding model using the correct wrapper\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Create Chroma vectorstore\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    embedding=embedding_model\n",
        ")\n",
        "\n",
        "# Create retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n"
      ],
      "metadata": {
        "id": "UvvLISm52Moa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Language Model (LLM) Initialization\n",
        "\n",
        "In this step, we load a lightweight open-source language model to generate natural-language responses based on the retrieved context.\n",
        "\n",
        "### **FLAN-T5 (Base Model)**\n",
        "We use the `google/flan-t5-base` model via HuggingFace's `pipeline`.  \n",
        "FLAN-T5 is a fine-tuned version of T5 that excels in:\n",
        "- instruction following  \n",
        "- text generation  \n",
        "- question answering  \n",
        "- summarization  \n",
        "\n",
        "This model is efficient and works well for RAG systems where the heavy lifting of retrieval is handled by the vector store.\n",
        "\n",
        "### **Pipeline Setup**\n",
        "The `text2text-generation` pipeline allows us to send prompts and receive generated answers.  \n",
        "`max_new_tokens=256` controls the maximum length of the model's output.\n"
      ],
      "metadata": {
        "id": "G4J4-kCHiNbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#second trail#\n",
        "from transformers import pipeline\n",
        "\n",
        "llm = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_new_tokens=256\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YuLo467p7uNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tavily-python\n"
      ],
      "metadata": {
        "id": "aGBGbstKuhFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "\n",
        "# You MUST set the key before creating TavilyClient\n",
        "os.environ[\"TAVILY_API_KEY\"] = \" \"\n",
        "\n",
        "tavily = TavilyClient()\n",
        "\n",
        "def internet_search(query):\n",
        "    result = tavily.search(query)\n",
        "    return result[\"results\"]\n",
        "\n"
      ],
      "metadata": {
        "id": "1t9qhDIvuhKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Logic: Combine RAG + Internet Search\n",
        "\n",
        "This function acts as the main agent responsible for answering user questions.  \n",
        "It follows a hybrid approach:\n",
        "\n",
        "1. **Try RAG first** ‚Äî If the answer exists in the embedded documents, return it immediately.  \n",
        "2. **Fallback to Web Search** ‚Äî If RAG cannot answer (e.g., \"I don't know\"), perform an online search.  \n",
        "3. **LLM Reasoning** ‚Äî The retrieved internet results are passed to the LLM to generate a final, clean answer.  \n",
        "\n",
        "This ensures that the agent always provides the most relevant information available.\n"
      ],
      "metadata": {
        "id": "BkIjIuagmqfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def agent(question):\n",
        "    # Step 1: Try RAG first\n",
        "    rag_response = rag_answer(question)\n",
        "\n",
        "    if \"I don't know\" not in rag_response:\n",
        "        return \"üìò Answer from PDF:\\n\\n\" + rag_response\n",
        "\n",
        "    # Step 2: If RAG fails ‚Üí use internet search\n",
        "    web_results = internet_search(question)\n",
        "\n",
        "    combined = \"\"\n",
        "    for item in web_results[:3]:  # take top 3\n",
        "        combined += item[\"title\"] + \"\\n\" + item[\"content\"] + \"\\n\\n\"\n",
        "\n",
        "    # Step 3: Ask LLM using the internet info\n",
        "    prompt = f'''\n",
        "Use ONLY the following internet search information to answer the question:\n",
        "\n",
        "{combined}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer clearly in 4‚Äì6 sentences.\n",
        "    '''\n",
        "    final = llm(prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return \"üåê Answer from Internet:\\n\\n\" + final\n"
      ],
      "metadata": {
        "id": "Kvpe7qnSuhNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(agent(\"What are the top e-commerce platforms in Saudi Arabia?\"))\n"
      ],
      "metadata": {
        "id": "4iA9_qVquwME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Answer Function (Primary Retrieval-Based Response)\n",
        "\n",
        "This function handles the Retrieval-Augmented Generation (RAG) workflow.  \n",
        "It attempts to answer the user's question *only* using information found in the embedded documents.\n",
        "\n",
        "**Steps performed:**\n",
        "\n",
        "1. **Retrieve Relevant Chunks**  \n",
        "   The retriever fetches the top matching text chunks based on semantic similarity.\n",
        "\n",
        "2. **Normalize Retriever Output**  \n",
        "   Ensures consistent formatting across different retriever APIs.\n",
        "\n",
        "3. **Clean and Deduplicate Context**  \n",
        "   Removes repeated lines and builds a concise context for the LLM.\n",
        "\n",
        "4. **Construct a Controlled Prompt**  \n",
        "   The LLM is instructed to answer *strictly* from the retrieved documents and respond  \n",
        "   `\"I don't know based on the provided documents.\"` if the answer is missing.\n",
        "\n",
        "5. **Generate the Final RAG Answer**  \n",
        "   The LLM produces a clear, short 4‚Äì6 sentence response grounded in the context.\n",
        "\n",
        "This function is the core of the RAG pipeline, providing reliable, document-based answers before the system attempts web search fallback.\n"
      ],
      "metadata": {
        "id": "64gwtYTMm7GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(question, k=4):\n",
        "    # 1. Retrieve documents (new API uses .invoke)\n",
        "    docs = retriever.invoke(question)\n",
        "\n",
        "    # Normalize retriever output\n",
        "    if isinstance(docs, dict):\n",
        "        docs = docs.get(\"documents\", [])\n",
        "    elif not isinstance(docs, list):\n",
        "        docs = [docs]\n",
        "\n",
        "    # If no relevant documents found\n",
        "    if not docs:\n",
        "        return \"I don't know based on the provided documents.\"\n",
        "\n",
        "    # 2. Build context (clean and remove duplicate lines)\n",
        "    raw_context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "    # Remove repeated lines\n",
        "    lines = raw_context.split(\"\\n\")\n",
        "    unique_lines = list(dict.fromkeys([line.strip() for line in lines if line.strip() != \"\"]))\n",
        "    context = \"\\n\".join(unique_lines)\n",
        "\n",
        "    # 3. Build the prompt for the LLM\n",
        "    prompt = f\"\"\"\n",
        "You are an AI assistant that answers questions about e-commerce and SMEs in Saudi Arabia.\n",
        "Use ONLY the information provided in the context below.\n",
        "If the answer is not in the context, say: \"I don't know based on the provided documents.\"\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a clear, short answer in 4‚Äì6 sentences without repeating the same lines.\n",
        "\"\"\"\n",
        "\n",
        "    # 4. Generate the answer\n",
        "    result = llm(prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return result.strip()\n"
      ],
      "metadata": {
        "id": "BCb_oaJ9VDaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_answer(\"What are the main challenges faced by SMEs when adopting e-commerce in Saudi Arabia?\"))\n"
      ],
      "metadata": {
        "id": "-Z3BNGKWVDc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(rag_answer(\"How does e-commerce support SME growth in Saudi Arabia?\"))\n"
      ],
      "metadata": {
        "id": "mAPIutmKVDfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building the Gradio User Interface\n"
      ],
      "metadata": {
        "id": "W5rdVanBnF-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_fn(question):\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question.\"\n",
        "    answer = rag_answer(question)\n",
        "    return answer\n",
        "\n",
        "# Custom CSS for professional styling\n",
        "custom_css = \"\"\"\n",
        ".gradio-container {\n",
        "    font-family: 'Inter', 'Segoe UI', sans-serif;\n",
        "    max-width: 1200px;\n",
        "    margin: 0 auto;\n",
        "}\n",
        "\n",
        ".header-container {\n",
        "    background: linear-gradient(135deg, #1a5f3f 0%, #2d8659 100%);\n",
        "    padding: 2.5rem 2rem;\n",
        "    border-radius: 12px;\n",
        "    margin-bottom: 2rem;\n",
        "    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".header-title {\n",
        "    color: white;\n",
        "    font-size: 2rem;\n",
        "    font-weight: 700;\n",
        "    margin-bottom: 0.5rem;\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "    gap: 0.75rem;\n",
        "}\n",
        "\n",
        ".header-subtitle {\n",
        "    color: rgba(255, 255, 255, 0.9);\n",
        "    font-size: 1rem;\n",
        "    font-weight: 400;\n",
        "}\n",
        "\n",
        "#question-box, #answer-box {\n",
        "    border-radius: 8px;\n",
        "    border: 1px solid #e0e0e0;\n",
        "    font-size: 0.95rem;\n",
        "}\n",
        "\n",
        "#question-box:focus, #answer-box:focus {\n",
        "    border-color: #2d8659;\n",
        "    box-shadow: 0 0 0 3px rgba(45, 134, 89, 0.1);\n",
        "}\n",
        "\n",
        ".submit-btn {\n",
        "    background: linear-gradient(135deg, #1a5f3f 0%, #2d8659 100%);\n",
        "    color: white;\n",
        "    border: none;\n",
        "    padding: 0.75rem 2rem;\n",
        "    border-radius: 8px;\n",
        "    font-weight: 600;\n",
        "    font-size: 1rem;\n",
        "    cursor: pointer;\n",
        "    transition: all 0.3s ease;\n",
        "    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        "\n",
        ".submit-btn:hover {\n",
        "    transform: translateY(-2px);\n",
        "    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15);\n",
        "}\n",
        "\n",
        ".info-card {\n",
        "    background: #f8f9fa;\n",
        "    border-left: 4px solid #2d8659;\n",
        "    padding: 1rem 1.5rem;\n",
        "    border-radius: 8px;\n",
        "    margin: 1.5rem 0;\n",
        "}\n",
        "\n",
        ".footer {\n",
        "    text-align: center;\n",
        "    color: #666;\n",
        "    font-size: 0.875rem;\n",
        "    margin-top: 2rem;\n",
        "    padding-top: 1.5rem;\n",
        "    border-top: 1px solid #e0e0e0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=custom_css, theme=gr.themes.Soft()) as demo:\n",
        "    # Header\n",
        "    gr.HTML(\"\"\"\n",
        "        <div class=\"header-container\">\n",
        "            <div class=\"header-title\">\n",
        "                 Saudi E-Commerce Insights AI Agent\n",
        "            </div>\n",
        "            <div class=\"header-subtitle\">\n",
        "                Powered by Monsha'at and Saudi E-Commerce Reports\n",
        "            </div>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Info card\n",
        "    gr.HTML(\"\"\"\n",
        "        <div class=\"info-card\">\n",
        "            <strong>üí° How to use:</strong> Ask questions about Saudi e-commerce trends, market insights,\n",
        "            regulations, statistics, or any data from the official reports.\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Main interface\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            query = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                placeholder=\"e.g., What are the main e-commerce trends in Saudi Arabia?\",\n",
        "                lines=3,\n",
        "                elem_id=\"question-box\"\n",
        "            )\n",
        "\n",
        "            submit = gr.Button(\n",
        "                \"üîç Get Answer\",\n",
        "                variant=\"primary\",\n",
        "                elem_classes=\"submit-btn\"\n",
        "            )\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            output = gr.Textbox(\n",
        "                label=\"Answer\",\n",
        "                lines=10,\n",
        "                elem_id=\"answer-box\",\n",
        "                show_copy_button=True\n",
        "            )\n",
        "\n",
        "    # Examples section\n",
        "    gr.Markdown(\"### üìù Example Questions\")\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            \"What is the size of the Saudi e-commerce market?\",\n",
        "            \"What are the key challenges facing e-commerce in Saudi Arabia?\",\n",
        "            \"What regulations govern e-commerce in Saudi Arabia?\",\n",
        "            \"What are the growth projections for Saudi e-commerce?\"\n",
        "        ],\n",
        "        inputs=query\n",
        "    )\n",
        "\n",
        "    # Footer\n",
        "    gr.HTML(\"\"\"\n",
        "        <div class=\"footer\">\n",
        "            <p>¬© 2024 Saudi E-Commerce Insights | Data sourced from official reports</p>\n",
        "        </div>\n",
        "    \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    submit.click(chat_fn, inputs=query, outputs=output)\n",
        "    query.submit(chat_fn, inputs=query, outputs=output)\n",
        "\n",
        "demo.launch(share=False, show_error=True)"
      ],
      "metadata": {
        "id": "yTSpCUpwf5sS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}